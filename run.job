#!/bin/tcsh
#PBS -l ngpus=1
#PBS -l ncpus=16
#PBS -l walltime=04:00:00
#PBS -q workq@e5-cse-cbgpu01.eecscl.psu.edu
#PBS -N pt
#PBS -M pxm5426@psu.edu 
#PBS -m bea
#PBS -l mem=100g
#PBS -o pbs_results/
#PBS -e pbs_results/
cd $PBS_O_WORKDIR

###
##  You may have set your $PATH variable depending on your situation -- if your error output can't find commands, you probably need to set your $PATH
#
##  If you have your working environment/shell set up to support running your code, you can try using "qsub -V job-name" to submit your job using all of your current environmental variables.
###
source ~/.tcshrc
conda activate llama-factory-env

# Resume from checkpoint
# llamafactory-cli train /scratch/pxm5426/runs/lora-exploration/llama-factory-1/Llama-3.2-3B_opc_edu,opc_evol,opc_mceval,opc_pkg_full_ebs128_lr1e-05/training_config.yaml 
# python -m torch.distributed.run \
#   --nproc_per_node 2 scripts/dump_optim_state.py \
#   --run_dir /scratch/pxm5426/runs/lora-exploration/llama-factory-1/Llama-3.2-3B_slimorca_full_ebs128_lr1e-05 \
#   --checkpoint checkpoint-2000  


python scripts/visualize_optim_state.py \
  --optim_state /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_math_reasoning_full_ebs128_lr5e-06/export_full_state_checkpoint-1200/optimizer_full.pt \
  --out_dir /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_math_reasoning_full_ebs128_lr5e-06/export_full_state_checkpoint-1200/optim_stats

# llamafactory-cli chat configs/tulu3_mix/inference-math-reasoning.yaml


# llamafactory-cli train examples/llama3.2_3b_full_sft_open_r1_math_galore.yaml

# python -m torch.distributed.run --nproc_per_node 2 scripts/dump_optim_state.py --run_dir /scratch/pxm5426/runs/lora-exploration/llama-factory/Llama-3.1-8B_tulu3_mixture_math_reasoning_full_ebs128_lr5e-06 --checkpoint checkpoint-1200