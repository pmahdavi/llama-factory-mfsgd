#!/bin/tcsh
#PBS -l ngpus=1
#PBS -l ncpus=8
#PBS -l walltime=01:00:00
#PBS -q workq@e5-cse-cbgpu01.eecscl.psu.edu
#PBS -N extract
#PBS -M pxm5426@psu.edu 
#PBS -m bea
#PBS -l mem=60g
#PBS -o pbs_results/
#PBS -e pbs_results/
cd $PBS_O_WORKDIR

###
##  You may have set your $PATH variable depending on your situation -- if your error output can't find commands, you probably need to set your $PATH
#
##  If you have your working environment/shell set up to support running your code, you can try using "qsub -V job-name" to submit your job using all of your current environmental variables.
###
source ~/.tcshrc
conda activate llama-factory-env

# Resume from checkpoint
# llamafactory-cli train /scratch/pxm5426/runs/lora-exploration/llama-factory-1/Llama-3.2-3B_opc_edu,opc_evol,opc_mceval,opc_pkg_full_ebs128_lr1e-05/training_config.yaml 
# python -m torch.distributed.run \
#   --nproc_per_node 2 scripts/dump_optim_state.py \
#   --run_dir /scratch/pxm5426/runs/lora-exploration/llama-factory-1/Llama-3.2-3B_slimorca_full_ebs128_lr1e-05 \
#   --checkpoint checkpoint-2000  


python scripts/visualize_optim_state.py \
  --optim_state /scratch/pxm5426/runs/lora-exploration/llama-factory-1/Llama-3.2-3B_slimorca_full_ebs128_lr1e-05/export_full_state/optimizer_full.pt \
  --out_dir /scratch/pxm5426/runs/lora-exploration/llama-factory-1/Llama-3.2-3B_slimorca_full_ebs128_lr1e-05/export_full_state/optim_stats